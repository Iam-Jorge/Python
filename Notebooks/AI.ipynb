{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inteligencia artificial\n",
    "![title](../img/TitleDivider.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Introducción](#t1)\n",
    "    - [1.1. ¿Qué es la IA?](#t1_1)\n",
    "    - [1.2. Agentes inteligentes](#t1_2)\n",
    "- [2. Resolver problemas mediante búsqueda](#t2)\n",
    "    - [2.1. Estrategias de búsqueda no informada](#t2_1)\n",
    "    - [2.2. Estrategias de búsqueda informada y exploración](#t2_2)\n",
    "    - [2.3. Algoritmos de búsqueda local y problemas de optimización](#t2_3)\n",
    "- [3. Problemas de satisfacción de restricciones](#t3)\n",
    "    - [3.1. Problemas de satisfacción de restricciones](#t3_1)\n",
    "    - [3.2. Búsqueda con vuelta atráspara PSR](#t3_2)\n",
    "    - [3.3. Resolución por Búsqueda Local](#t3_3)\n",
    "- [4. Búsqueda entre adversarios.](#t4)\n",
    "    - [4.1. Desiciones optimas](#t4_1)\n",
    "    - [4.2. Poda Alfa-Beta](#t4_2)\n",
    "    - [4.3. Funciones de evaluación](#t4_3)\n",
    "- [5. Aprendizaje](#t5)\n",
    "    - [5.1. Aprendizaje supervisado](#t5_1)\n",
    "    - [5.2. Aprendizaje no supervisado](#t5_2)\n",
    "    - [5.3. Aprendizaje por refuerzo](#t5_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción <a class=\"anchor\" id=\"t1\"></a>\n",
    "![title](../img/Divider.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante miles de años, hemos tratado de entender cómo pensamos; es decir, entender cómo un simple puñado de materia puede percibir, entender, predecir y manipular un mundo mucho más grande y complicado que ella misma. El campo de la inteligencia artificial,\n",
    "o IA, va más allá: no sólo intenta comprender, sino que también se esfuerza en construir entidades inteligentes.\n",
    "\n",
    "La IA es una de las ciencias más recientes. El trabajo comenzó poco después de la Segunda Guerra Mundial, y el nombre se acuñó en 1956.\n",
    "\n",
    "La IA abarca en la actualidad una gran variedad de subcampos, la IA **sintetiza y automatiza** tareas intelectuales y es, por lo tanto, potencialmente relevante para cualquier ámbito de la actividad intelectual humana. En este sentido, es un campo genuinamente universal.\n",
    "\n",
    "Una breve historia de las disciplinas que han contribuido con ideas, puntos de vista y técnicas al desarrollo en torno a una serie de cuestiones, con el objetivo último de todas estas disciplinas en hacer avanzar la IA.\n",
    "\n",
    "- Filosofía (desde el año 428 a.C. hasta el presente)\n",
    "- Matemáticas (aproximadamente desde el año 800 al presente)\n",
    "- Economía (desde el año 1776 hasta el presente)\n",
    "- Neurociencia (desde el año 1861 hasta el presente)\n",
    "- Psicología (desde el año 1879 hasta el presente)\n",
    "- Ingeniería computacional (desde el año 1940 hasta el presente)\n",
    "- Teoría de control y cibernética (desde el año 1948 hasta el presente)\n",
    "- Lingüística (desde el año 1957 hasta el presente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Historia (Breve) de la inteligencia artificial**\n",
    "\n",
    "- 1940-1950: Primeros días de la IA●1943: McCulloc & Pitts: Modelo Booleano del Cerebro.\n",
    "    - 1950: Turing: Computing Machinery and Intelligence.\n",
    "- 1950-1970: Mira Mamá, Sin Manos!!\n",
    "    - 1950s: Primeros programas de IA (Samuells, Newell,...)\n",
    "    - 1956: Adopción del término “Inteligencia Artificial”.\n",
    "    - 1965: Algoritmo de Robinson de Razonamiento Lógico.\n",
    "-  1970-1990: Sistemas Basados Conocimiento\n",
    "    - 1970s: Primeros sistemas basados en el Conocimiento.\n",
    "    - 1980s: Boom de los Sistemas Expertos.\n",
    "    - 1993: Pesimismo Sistemas Expertos: AI-Winter.\n",
    "- 1990-Hoy: Aproximaciones Modernas\n",
    "    - Probabilidad, Estadística e Incertidumbre.\n",
    "    - Incremento general en profundidad técnica.\n",
    "    - Agentes , Agentes , Agentes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. ¿Qué es la IA? <a class=\"anchor\" id=\"t1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lo largo de la historia se han seguido los cuatro enfoques representados en la siguiente figura:\n",
    "\n",
    "<center><img src=\"../img/AI-Figura1-1.jpeg\"/></center>\n",
    "\n",
    "Las que aparecen en la parte superior se refieren a **_procesos mentales y al razonamiento_**, mientras que los de la parte inferior aluden a la **_conducta_**.\n",
    "Las definiciones de la izquierda miden el éxito en términos de la fidelidad en la forma de actuar de los **_humanos_**, mientras que las de la derecha toman como referencia un concepto ideal de inteligencia, que llamaremos **_racionalidad_**.\n",
    "\n",
    "> Un sistema es racional si hace «lo correcto», en función de su conocimiento.\n",
    "\n",
    "- Piensan como Humanos (Ciencia cognitiva)\n",
    "- Piensan racionalmente (Leyes del pensamiento)\n",
    "- Actúan como humanos (Prueba de Turing)\n",
    "- Actúan Racionalmente (Agente Racional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comportamiento humano: el enfoque de la Prueba de Turing\n",
    "\n",
    "La Prueba de Turing, propuesta por Alan Turing (1950), se diseñó para proporcionar una definición operacional y satisfactoria de inteligencia. En vez de proporcionar una lista larga y quizá controvertida de cualidades necesarias para obtener inteligencia artificialmente, él sugirió una prueba basada en la incapacidad de diferenciar entre entidades inteligentes indiscutibles y seres humanos.\n",
    "\n",
    "El computador supera la prueba si un evaluador (humano) no es capaz de distinguir si las respuestas, a una serie de preguntas planteadas, son de una persona o no.\n",
    "\n",
    "Para superar la prueba de Turing el computador debería poseer las siguientes capacidades:\n",
    "- Procesamiento de lenguaje natural que le permita comunicarse satisfactoriamente en inglés.\n",
    "- Representación del conocimiento para almacenar lo que se conoce o siente.\n",
    "- Razonamiento automático para utilizar la información almacenada para responder a preguntas y extraer nuevas conclusiones.\n",
    "- Aprendizaje automático para adaptarse a nuevas circunstancias y para detectar y extrapolar patrones.\n",
    "\n",
    "El test global añade:\n",
    "- Visión computacional para percibir objetos.\n",
    "- Robótica para manipular y mover objetos.\n",
    "\n",
    "Estas seis disciplinas abarcan la mayor parte de la IA, y Turing merece ser reconocido por diseñar una prueba que se conserva vigente después de 50 años."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pensar como un humano: el enfoque del modelo cognitivo\n",
    "\n",
    "Para poder decir que un programa dado piensa como un humano, es necesario contar con un mecanismo para determinar cómo piensan los humanos. Es necesario penetrar en el funcionamiento de las mentes humanas. Hay dos formas de hacerlo:\n",
    "- Mediante introspección (intentando atrapar nuestros propios pensamientos conforme éstos van apareciendo).\n",
    "- Mediante experimentos psicológicos.\n",
    "\n",
    "Una vez se cuente con una teoría lo suficientemente precisa sobre cómo trabaja la mente, se podrá expresar esa teoría en la forma de un programa de computador.\n",
    "\n",
    "Si los datos de entrada/salida del programa y los tiempos de reacción son similares a los de un humano, existe la evidencia de que algunos de los mecanismos del programa se pueden comparar con los que utilizan los seres humanos. No basta con que el programa resielva correctamente los problemas, lo que interesa es seguir la pista de las etapas del proceso de razonamiento y compararlas con las seguidas por humanos a los que se les enfrentó a los mismos problemas. En el campo de la **_ciencia cognitiva_** convergen modelos computacionales de IA y técnicas experimentales de psicología intentando elaborar teorías precisas y verificables sobre el funcionamiento de la mente humana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pensamiento racional: el enfoque de las «leyes del pensamiento»\n",
    "\n",
    "El filósofo griego Aristóteles fue uno de los primeros en intentar codificar la «manera correcta de pensar», es decir, un proceso de razonamiento irrefutable. Sus silogismos son esquemas de estructuras de argumentación mediante las que siempre se llega a conclusiones correctas si se parte de premisas correctas. (por ejemplo: «Sócrates es un hombre; todos los hombres son mortales; por lo tanto Sócrates es mortal»). Estas leyes de pensamiento supuestamente gobiernan la manera de operar de la mente; su estudio fue el\n",
    "inicio de un campo llamado **_lógica_**.\n",
    "\n",
    "En el siglo XIX, se desarrollo una notación precisa para definir sentencias sobre todo tipo de elementos del mundo y especificar relaciones entre ellos (compárese esto con la notación aritmética). Ya en 1965 existían programas que, en principio, resolvían cualquier problema resoluble descrito en notación lógica. La llamada tradición logista dentro del campo de la inteligencia artificial trata de construir sistemas inteligentes a partir de estos programas. Este enfoque presenta dos obstáculos:\n",
    "- No es fácil transformar conocimiento informal y expresarlo en los términos formales que requieren de notación lógica.\n",
    "- Hay una gran diferencia entre poder resolver un problema «en principio» y hacerlo en la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Agentes inteligentes <a class=\"anchor\" id=\"t1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **agente** es cualquier entidad autónoma (software, hardware o ambas) capaz de percibir su entorno con la ayuda de **sensores** y actuar en ese entorno utilizando **actuadores**. El termino **percepción** se utiliza en este contexto para indicar que el agente puede recibir entradas en cualquier instante. La **secuencia de percepciones** de un agente refleja el historial completo de lo que el agente ha recibido.\n",
    "\n",
    "Un **agente racional**:\n",
    "- Hace lo correcto para obtener el mejor resultado.\n",
    "- Necesita definir una medida de rendimiento para obtener el mejor resultado.\n",
    "- Una medida de rendimiento evalua la secuencia de entorno.\n",
    "\n",
    "<center><img src=\"../img/AgenteIA.jpeg\"/></center>\n",
    "\n",
    "\n",
    "La función que describe el comportamiento de un agente se puede presentar en forma de tabla. Suponer un mundo hecho a medida para una aspiradora, que tiene solamente dos localizaciones: cuadrícula A y cuadrícula B. La aspiradora puede:\n",
    "- Percibir en qué cuadrante se encuentra.\n",
    "- Percibir si hay suciedad en el cuadrante. \n",
    "- Puede elegir si se mueve a la derecha, izquierda, aspirar la suciedad o no hacer nada.  \n",
    "\n",
    "Una función muy simple vendría dada por: si la cuadrícula en la que se encuentra está sucia, entonces aspirar, de otra forma, cambiar de cuadrícula.\n",
    "\n",
    "<center><img src=\"../img/AgenteAspirador.jpeg\"/></center>\n",
    "\n",
    "**Propiedades de un agente**\n",
    "- **Autonomía**: Es independiente, se apoya en el entorno.\n",
    "- **Sociabilidad**: Se comunica con otros iguales.\n",
    "- **Reactividad** Reacciona ante eventos.\n",
    "- **Proactividad**: Es capaz de promover acciones. \n",
    "- **Adaptabilidad**: Se pueden desenvolver ante cambios en el entorno sin cambiar el agente. \n",
    "- **Movilidad**: Es capaz de realizar acciones.\n",
    "- **Veracidad**: Si un agente tiene que hacer una acción realizara solo esa acción.\n",
    "- **Racionalidad**: Actúa racionalmente para que lo haga lo mejor posible de acuerdo a unos criterios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Buen comportamiento: el concepto de racionalidad**\n",
    "\n",
    "“Actuar racionalmente consiste en realizar el conjunto de acciones que lleven a cumplir un objetivo de la mejor manera con el conocimiento que se tiene”. Por tanto, el comportamiento racional no requiere necesariamente de pensamiento (deliberación), y es totalmente independiente de las metas.\n",
    "\n",
    "La Racionalidad implica:\n",
    "- Exploración para recopilar información.\n",
    "- Aprendizaje de lo que percibe.\n",
    "- Autonomia para completar conocimiento parcial.\n",
    "\n",
    "**_Racionalidad_**\n",
    "\n",
    "En el caso del enfoque de la IA según las «leyes del pensamiento», todo el énfasis se pone en hacer inferencias correctas. Una manera racional de actuar es llegar a la conclusión lógica de que si una acción dada permite alcanzar un objetivo. Sin embargo, el efectuar una inferencia correcta no depende siempre de la racionalidad, ya que existen situaciones para las que no hay nada correcto que hacer y en las que hay que tomar una decisión.\n",
    "\n",
    "¿Qué significa hacer lo correcto? Como primera aproximación se puede decir que lo correcto es aquello que permite al agente obtener el mejor resultado posible.\n",
    "\n",
    "La racionalidad en un instante dado depende de cuatro factores:\n",
    "- La medida de rendimiento que define el criterio de exito.\n",
    "- El conocimiento del medio que tiene el agente.\n",
    "- Las acciones que el agente puede realizar.\n",
    "- La secuencia de percepciones recibida.\n",
    "\n",
    "Obtener una racionalidad perfecta (hacer siempre lo correcto) no es posible en entornos complejos. La demanda computacional que esto implica es demasiado grande. Se adoptará la hipótesis de trabajo de que la racionalidad perfecta es un buen punto de partida para el análisis. Lo cual simplifica el problema y proporciona el escenario base adecuado. Cuando no se cuenta con el tiempo suficiente para efectuar todos los cálculos deseables **_la racionalidad limitada_** nos permite actuar adecuadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Entorno de trabajo_**\n",
    "\n",
    "Para diseñar un agente Racional el primer paso debe ser siempre especificar un Entorno de Trabajo de la forma más completa posible: REAS (Rendimiento, Entorno, Actuadores, Sensores).\n",
    "\n",
    "- **Rendimiento**. Cualidades deseables que el agente debería tener. Seguro, rápido, legal, viaje confortable, maximiza el beneficio.\n",
    "- **Entorno**. ¿Dónde se va a desenvolver nuestro agente? Carreteras, tráfico, peatones, clientes, tiempo (clima).\n",
    "- **Actuadores**. Elementos que nos permitan modificar ese entorno. Dirección, acelerador, freno, señal, bocina, visualizador...\n",
    "- **Sensores**. Permiten conocer la situación instantánea del entorno. Cámaras, sónar, GPS, tacómetro, acelerómetro, sensores del motor. \n",
    "\n",
    "**Propiedades de los entornos de trabajo**\n",
    "- **Totalmente observable vs. parcialmente observable**: Un entorno de trabajo es, efectivamente, totalmente observable si los sensores detectan todos los aspectos que son relevantes en la toma de decisiones.\n",
    "- **Determinista vs. estocástico**: Si el siguiente estado del medio está totalmente determinado por el estado actual y la acción ejecutada por el agente, entonces se dice que el entorno es determinista.\n",
    "- **Episódico vs. secuencial**: En un entorno de trabajo episódico, la experiencia del agente se divide en episodios atómicos, cada episodio consiste en la percepción del agente y la realización de una única acción posterior, donde cada elección de la acción en cada episodio depende sólo del episodio en sí mismo. En entornos secuenciales, por otro lado, la decisión presente puede afectar a decisiones futuras.\n",
    "- **Estático vs. dinámico**: En los nedios estáticos el agnete no necesita estar pendiente del mundo mientras está tomando una decisión, ni necesita preocuparse sobre el paso del tiempo. Los medios dinámicos, por el contrario, están preguntando continuamente al agente qué quiere hacer. Si el entorno no cambia con el paso del tiempo, pero el rendimiento del agente cambia, entonces se dice que el medio es semidinámico.\n",
    "- **Discreto vs. continuo**: La distinción entre discreto y continuo se puede aplicar al estado del medio, a la forma en la que se maneja el tiempo y a las percepciones y acciones del agente.\n",
    "- **Agente individual vs. multiagente**: La distinción entre el entorno de un agente individual y el de un sistema multiagente puede parecer suficientemente simple. Por ejemplo, un agente resolviendo un crucigrama por sí mismo está claramente en un entorno de agente individual, mientras que un agente que juega al ajedrez está en un entorno con dos agentes.\n",
    "\n",
    "La siguiente ficura presenta las propiedades de un número de entornos:\n",
    "\n",
    "<center><img src=\"../img/EntornosDeTrabajo.jpeg\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tipos de agentes**\n",
    "\n",
    "_¿Cómo trabaja internamente un agente?_\n",
    "\n",
    "El trabajo de la IA es diseñar el **programa del agente** que implemente la función del agente que proyecta las percepciones en las acciones. Se asume que este programa se ejecutará en algún tipo de computador con sensores físicos y actuadores, lo cual se conoce como **arquitectura**: \n",
    "\n",
    "**_Agente = arquitectura + programa_**\n",
    "\n",
    "Los programas de los agentes reciben las percepciones actuales como entradas de los sensores y devuelven una acción a los actuadores. Hay que tener en cuenta la diferencia entre los programas de los agentes, que toman la percepción actual como entrada, y la función del agente, que recibe la percepción histórica completa.\n",
    "\n",
    "Tipos básicos de programas para agentes que encarnan los principios que subyacen en casi todos los sistemas inteligentes:\n",
    "\n",
    "**_- Agentes reactivos simples._**\n",
    "El tipo de agente más sencillo es el agente reactivo simple. Estos agentes seleccionan las acciones sobre la base de las percepciones actuales, ignorando el resto de las percepciones históricas. \n",
    "\n",
    "**_- Agentes reactivos basados en modelos._**\n",
    "Un agente reactivo simple con estado interno, muestra cómo la percepción actual se combina con el estado interno antiguo para generar la descripción actualizada del estado actual. Además de interpretar la nueva percepción a partir del conocimiento existente sobre el estado, utiliza información relativa a la forma en la que evoluciona el mundo.\n",
    "La actualización del estado según pasa el tiempo requiere codificar dos tipos de conocimiento en el programa del agente.\n",
    "- Primero, se necesita alguna información acerca de cómo evoluciona el mundo independientemente del agente.\n",
    "- Segundo, se necesita más información sobre cómo afectan al mundo las acciones del agente.\n",
    "\n",
    "**_- Agentes basados en objetivos._** \n",
    "Es unn agente basado en objetivos y basado en modelos, que almacena información del estado del mundo, así como del conjunto de objetivos que intenta alcanzar, y que es capaz de seleccionar la acción que eventualmente lo guiará hacia la consecución de sus objetivos.\n",
    "\n",
    "**_- Agentes basados en utilidad._** \n",
    "Es un agente basado en utilidad y basado en modelos utiliza un modelo del mundo, junto con una función de utilidad que calcula sus preferencias entre los estados del mundo.\n",
    "- Primero, cuando haya objetivos conflictivos, y sólo se puedan alcanzar algunos de ellos.\n",
    "- Segundo, cuando haya varios objetivos por los que se pueda guiar el agente, y ninguno de ellos se pueda alcanzar con certeza.\n",
    "\n",
    "**_- Agentes que aprenden._** \n",
    "Un agente que aprende se puede dividir en cuatro componentes conceptuales. La distinción más entre el **elemento de aprendizaje** y el **elemento de actuación** es que el primero está responsabilizado de hacer mejoras y el segundo se responsabiliza de la selección de acciones externas. \n",
    "\n",
    "El elemento de aprendizaje se realimenta con las críticas sobre la actuación del agente y determina cómo se debe modificar el elemento de actuación para proporcionar mejores resultados en el futuro.\n",
    "\n",
    "La crítica indica al elemento de aprendizaje qué tal lo está haciendo el agente con respecto a un nivel de actuación fijo,  la percepción por sí misma no lo indica.\n",
    "\n",
    "El generador de problemas es responsable de sugerir acciones que lo guiarán hacia experiencias nuevas e informativas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resolver problemas mediante búsqueda<a class=\"anchor\" id=\"t2\"></a>\n",
    "![title](../img/Divider.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se describe una clase de **agente basado en objetivos** llamado agente **resolvente-problemas**, que deciden qué hacer para encontrar secuencias de acciones que conduzcan a los estados deseables. Se describe con precisión los elementos que constituyen el **problema** y su **solución**. Posteriormente, se describen los algoritmos de propósito general que podamos utilizar para resolver estos problemas y así comparar las ventajas de cada algoritmo. \n",
    "\n",
    "Los problemas de búsqueda implican un agente al que se le da un _estado inicial_ y un _estado objetivo_, y que devuelve una solución de cómo llegar del primero al segundo. Una aplicación de navegación utiliza un proceso de búsqueda típico, en el que el agente (la parte pensante del programa) recibe como datos de entrada tu ubicación actual y tu destino deseado y, basándose en un algoritmo de búsqueda, devuelve un camino sugerido. Sin embargo, hay muchas otras formas de problemas de búsqueda, como rompecabezas o laberintos.\n",
    "\n",
    "Para encontrar la solución a un 15 rompecabezas habría que utilizar un algoritmo de búsqueda.\n",
    "\n",
    "<center><img src=\"../img/SearchPuzzle.jpeg\"/></center>\n",
    "\n",
    "- **Agente**: Entidad que percibe su entorno y actúa sobre él. En una aplicación de navegación, por ejemplo, el agente sería la representación de un coche que debe decidir qué acciones realizar para llegar a su destino.\n",
    "- **Estado**: Configuración de un agente en su entorno. Por ejemplo, en un puzzle del 15, un estado es cualquier forma en la que todos los números están dispuestos en el tablero.\n",
    "- **Acciones**: Opciones que se pueden tomar en un estado. Más concretamente, al recibir el estado s como entrada, Actions(s) devuelve como salida el conjunto de acciones que se pueden ejecutar en el estado s.\n",
    "\n",
    "Un problema puede definirse formalmente por cinco componentes: \n",
    "- **Estado inicial**: Estado del que parte el algoritmo de búsqueda. En una aplicación de navegación, sería la ubicación actual.\n",
    "- **Función sucesor**: Una descripción de qué estado resulta de realizar cualquier acción aplicable en cualquier estado. Más concretamente, al recibir el estado s y la acción a como entrada, Results(s, a) devuelve el estado resultante de realizar la acción a en el estado s.\n",
    "- **Estados o espacio de estados**: Conjunto de todos los estados alcanzables desde el estado inicial mediante cualquier secuencia de acciones. El espacio de estados puede visualizarse como un grafo dirigido con estados, representados como nodos, y acciones, representadas como flechas entre nodos.\n",
    "\n",
    "<center><img src=\"../img/SearchPuzzle2.jpeg\"/></center>\n",
    "\n",
    "- **Función objetivo**: Condición que determina si un estado dado es un estado objetivo.\n",
    "- **Coste del camino**: Coste numérico asociado a una ruta determinada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resolución de problemas de búsqueda**\n",
    "\n",
    "- **Solución**: Secuencia de acciones que dirigen el sistema desde un estado inicial a un estado que verifique la prueba objetivo.\n",
    "- **Solución óptima**: Solución que tiene el menor coste de camino entre todas las soluciones.\n",
    "\n",
    "> Nota: No confundir unicidad de la solución con solución óptima. La solución óptima es aquella que tiene el costo más pequeño, pero puede haber muchas más soluciones que tengan el mismo costo. \n",
    "\n",
    "\n",
    "**_Búsqueda de soluciones_**\n",
    "\n",
    "Esto se hace mediante búsqueda a través del espacio de estados. Nos centraremos en búsquedas que utilizan un **árbol de búsqueda** explícito generado por el estado inicial (nodo) y la función sucesor, definiendo así el espacio de estados. En general, podemos tener un **grafo de búsqueda** más que un árbol, cuando el mismo estado puede alcanzarse desde varios caminos.\n",
    "\n",
    "En un proceso de búsqueda, los datos suelen almacenarse en los nodos de un árbol o grafo de búsqueda, una estructura de datos, donde cada nodo contiene los siguientes datos:\n",
    "- Un estado.\n",
    "- Su nodo padre, a través del cual se generó el nodo actual.\n",
    "- La acción que se aplicó al estado del padre para llegar al nodo actual.\n",
    "- El coste del camino desde el estado inicial hasta este nodo.\n",
    "\n",
    "Los nodos contienen información que los hace muy útiles para los algoritmos de búsqueda. Contienen un estado, que se puede comprobar mediante el test objetivo para ver si es el estado final. Si lo es, el coste del camino del nodo puede compararse con los costes de los caminos de otros nodos, lo que permite elegir la solución óptima,\n",
    "\n",
    "Sin embargo, los nodos son simplemente una estructura de datos: no buscan, sino que guardan información. Para buscar realmente, utilizamos **estrategia de búsqueda**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Estrategias de búsqueda no informada <a class=\"anchor\" id=\"t2_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las estrategias de **búsqwueda no informada** (llamada también **búsqueda a ciegas**) englobadas cinco estrategias de búsqueda. El término significa que ellas no tienen información adicional acerca de los estados más allá de la que proporciona la definición del problema. Todo lo que ellas pueden hacer es generar los sucesores y distinguir entre un estado objetivo de uno que no lo es. Las estrategias que saben si un estado no objetivo es «más prometedor» que otro se llaman **búsqueda informada** o **búsqueda heurística**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Búsqueda en profundidad (Depth-First Search)**\n",
    "\n",
    "Un algoritmo de búsqueda en profundidad agota cada una de las direcciones antes de intentar otra dirección. Siempre expande el nodo más profundo en la frontera actual del árbol de búsqueda. \n",
    "\n",
    "> Frontera: Colección de nodos que se han generado pero todavía no se han expandido.\n",
    "\n",
    "Cuando esos nodos se expanden, son quitados de la frontera, así entonces la búsqueda retrocede al siguiente nodo más superficial que todavía tenga sucesores inexplorados. En estos casos, la frontera se gestiona como una estructura de datos en en pila LIFO (último en entrar primero en salir). \n",
    "\n",
    "<center><img src=\"../img/BProfundidad.jpeg\"/></center>\n",
    "\n",
    "Búsqueda primero en profundidad sobre un árbol binario. Los nodos que se han expandido y no tienen descendientes en la frontera se pueden quitar de la memoria; estos se muestran en negro. Los nodos a profundidad 3 se suponen que no tienen sucesores y M es el nodo objetivo\n",
    "\n",
    "Ventajas:\n",
    "- En el mejor de los casos, este algoritmo es el más rápido. Si \"tiene suerte\" y siempre elige el camino correcto hacia la solución (por casualidad), entonces la búsqueda en profundidad es la que menos tiempo tarda en llegar a la solución.\n",
    "Contras:\n",
    "- Es posible que la solución encontrada no sea óptima.\n",
    "- En el peor de los casos, este algoritmo explorará todos los caminos posibles antes de encontrar la solución, por lo que tardará el mayor tiempo posible en llegar a ella."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Búsqueda en anchura (Breadth-First Search)**\n",
    "\n",
    "La búsqueda en anchura es una estrategia sencilla en la que se expande primero el nodo raíz, a continuación se expanden todos los sucesores del nodo raíz, después sus sucesores, etc.\n",
    "\n",
    "La frontera es una estructura de cola FIFO (Primero en entrar, primero en salir). La cola FIFO pone todos los nuevos sucesores generados al final de la cola, lo que significa que los nodos más superficiales se expanden antes que los nodos más profundos. \n",
    "\n",
    "\n",
    "<center><img src=\"../img/BAnchura.jpeg\"/></center>\n",
    "\n",
    "Búsqueda primero en anchura sobre un árbol binario sencillo. En cada etapa, el próximo nodo a expandir se indica con una marca.\n",
    "\n",
    "Ventajas:\n",
    "- Este algoritmo garantiza encontrar la solución óptima.\n",
    "Desventajas:\n",
    "- Está casi garantizado que este algoritmo tarda más que el tiempo mínimo en ejecutarse.\n",
    "- En el peor de los casos, este algoritmo tarda el mayor tiempo posible en ejecutarse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Búsquedade costo uniforme**\n",
    "\n",
    "En vez de expandir el nodo más superficial, la búsquedade costo uniforme expande el nodo n con el camino de costo más pequeño. Notemos que, si todos los costos son iguales, es idéntico a la búsqueda en anchura. La búsqueda de costo uniforme no se preocupa por el número de pasos que tiene un camino, pero sí sobre su coste total. Podemos garantizar completitud si el costo de cada paso es  mayor o igual a alguna constante positiva pequeña e. Esta condiciónes también suficiente para asegurar optimización. Significa que el costo de un camino siempre aumenta cuando vamos por él. De esta propiedad, es fácil ver que el algoritmo expande nodos que incrementan el coste del camino. Por lo tanto, el primer nodo objetivo seleccionado para la expansión es la solución óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Búsqueda en profundidad acotada**\n",
    "\n",
    "Busca aliviar el problema de árboles ilimitados aplicando la búsquedaacotada en profundidad con un límitede profundidad L predeterminado. Es decir, los nodos a profundidad L se trataráncomo si no tuvieran ningúnsucesor. El límitede profundidad resuelve  el problema del camino infinito. Lamentablemente, tambiénintroduce una fuente adicional de incompletitud si escogemos un L < d, es decir, el objetivo está fuera del límite de profundidad.\n",
    "\n",
    "A veces, los límitesde profundidad puedenestar basados en el conocimiento del problema. Este número, conocido como **diámetro del espacio de estados**, nos da un mejor límitede profundidad, que conduce a una búsquedacon profundidad limitada más eficiente. Para la mayor parte de problemas, sin embargo, no conoceremos un límitede profundidad bueno hasta que hayamos resuelto el problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Búsqueda en profundidad iterativa**\n",
    "\n",
    "La búsqueda en profundidad iterativa es una estrategia que encuentra el mejor límite de profundidad. Esto se consigue aumentando gradualmente el límite (primero 0, después 1, después 2, etcétera) hasta que encontramos un objetivo. \n",
    "\n",
    "La búsqueda en profundidad iterativa combina las ventajas de la búsqueda en profundidad simple y la búsqueda en anchura. \n",
    "\n",
    "En la búsqueda en profundidad, sus exigencias de memoria son muy modestas: O(bd). La búsqueda en anchura es completa cuando el factor de ramificación es finito y óptima cuando el coste del camino es una función que no disminuye con la profundidad del nodo. \n",
    "\n",
    "<center><img src=\"../img/BProfundidadIterativa.jpeg\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resumen búsquedas no informadas**\n",
    "\n",
    "<center><img src=\"../img/EstrategiasBusqueda.jpeg\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evitar estados repetidos**\n",
    "\n",
    "Hasta este punto, casi hemos ignorado una de las complicaciones más importantes al proceso de búsqueda: la posibilidad de perder tiempo expandiendo estados que ya han sido visitados y expandidos. \n",
    "\n",
    "Para algunos problemas, esta posibilidad nunca aparece, el espacio de estados es un árbol y hay sólo un camino a cada estado. Para otros es inevitable, esto incluye todos los problemas donde las acciones son reversibles, como son los problemas de búsqueda de rutas y los puzles que deslizan sus piezas. Los árboles de la búsqueda para estos problemas son infinitos.\n",
    "\n",
    "Entonces, si el algoritmo no detecta los estados repetidos, éstos pueden provocar que un problema resoluble llegue a ser irresoluble. La detección por lo general significa la comparación del nodo a expandir con aquellos que han sido ya expandidos; si se encuentra\n",
    "un emparejamiento, entonces el algoritmo ha descubierto dos caminos al mismo estado y puede desechar uno de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Búsqueda con información parcial**\n",
    "\n",
    "Partimos desde un supuesto de que el entorno es totalmente observable y determinista y que el agente conoce cuáles son los efectos de cada acción. Por lo tanto, el agente puede calcular exactamente cuál es el estado resultado de cualquier secuencia de acciones y siempre sabe en qué estado está. Su percepción no proporciona ninguna nueva información después de cada acción. \n",
    "\n",
    "_¿Qué pasa cuando el conocimiento de los estados o acciones es incompleto?_\n",
    "\n",
    "Encontramos que diversos tipos de incompletitud conducen a tres tipos de problemas distintos:\n",
    "- **Problemas sin sensores** (también llamados problemas conformados): Si el agente no tiene ningún sensor, entonces (por lo que sabe) podría estar en uno de los posibles estados iniciales, y cada acción por lo tanto podría conducir a uno de los posibles estados sucesores.\n",
    "- **Problemas de contingencia**: Si el entorno es parcialmente observable o si las acciones son inciertas, entonces las percepciones del agente proporcionan nueva información después de cada acción. Cada percepción posible define una contingencia que debe de planearse. A un problema se le llama entre adversarios si la incertidumbre está causada por las acciones de otro agente.\n",
    "- **Problemas de exploración**: Cuando se desconocen los estados y las acciones del entorno, el agente debe actuar para descubrirlos. Los problemas de exploración pueden verse como un caso extremo de problemas de contingencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Estrategias de búsqueda informada y exploración <a class=\"anchor\" id=\"t2_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las estrategias de búsqueda **no informadas** pueden encontrar soluciones a problemas generando sistemáticamente nuevos estados y evaluando hasta alcanzar un objetivo. \n",
    "\n",
    "Lamentablemente, estas estrategias son increíblemente ineficientes en la mayoría de casos:\n",
    "- No consideran informción sobre estados y objetivos para decidir que camino expandir primero en la frontera.\n",
    "- Son estrategias generales y no consideran características específicas del problema.\n",
    "- Los algoritmos de busqueda no tienen en cuenta el objetivo hasta que están en el nodo objetivo.\n",
    "- En algunos problemas, existe información extra que puede ser usadapara guiar la búsqueda.\n",
    "\n",
    "Una estrategia de **búsqueda informada** puede encontrar soluciones de una manera más eficiente que una estrategia no informada. A la aproximación general se le conoce como **búsqueda primero el mejor**, que es un caso particular del algoritmo general de búsqueda-árboles o de búsqueda-grafos en el cual se selecciona un nodo para la expansión basada en una **función de evaluación**. Se implementa con con una cola con prioridad, expandiendo así siempre el nodo de la frontera que parece ser mejor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Búsqueda voraz primero el mejor**\n",
    "\n",
    "La búsqueda voraz primero el mejor trata de expandir el nodo más cercano al objetivo, alegando que probablemente conduzca rápidamente a una solución. Así, evalúa los nodos utilizando solamente la función heurística f(n) = h(n) en cada nodo, la función estima lo cerca de la meta que está el siguiente nodo. La eficacia del algoritmo depende de la calidad de la función heurística.\n",
    "\n",
    "_Ejemplo:_\n",
    "\n",
    "La siguiente figura muestra el progreso de una búsqueda primero el mejor voraz con hDLR para encontrar un camino desde Arad a Bucarest. El primer nodo a expandir desde Arad será Sibiu, porque está mas cerca de Bucarest que Zerind o Timisoara. El siguiente nodo a expandir será Fagaras, porque es la más cercana. Fagaras en su turno genera Bucarest, que es el objetivo. \n",
    "\n",
    "<center><img src=\"../img/BVoraz.jpeg\"/></center>\n",
    "\n",
    "_Propiedades:_\n",
    "- Completa: No, puede quedarse atascada en bucles. Completa en un espacio finito con comprobación de estados repetidos.\n",
    "- Temporal: O(b^m), pero una buena heurística puede mejorarla mucho.\n",
    "- Espacio: O(b^m), porque mantiene todos los nodos en memoria.\n",
    "- Optima: No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-Búsqueda A\\*: Minimiza el costo estimado total de la solución**\n",
    "\n",
    "La búsqueda A*, una evolución del algoritmo de búsqueda del mejor primero, tiene en cuenta lo siguiente: \n",
    "\n",
    ">   f(n) = **g(n) + h(n)** <= C*\n",
    "\n",
    "- h(n): El coste estimado desde la posición actual hasta la meta.\n",
    "- g(n): El coste acumulado hasta la posición actual.\n",
    "- f(n): El coste estimado total del camino que llega al objetivo pasando por n.\n",
    "- C*: El coste de la supuesta solución óptima.\n",
    "\n",
    "Al combinar ambos valores, el algoritmo dispone de una forma más precisa de determinar el coste de la solución y optimizar sus elecciones sobre la marcha. El algoritmo lleva la cuenta de (coste del camino hasta ahora + coste estimado hasta la meta), y una vez que supera el coste estimado de alguna opción anterior, el algoritmo abandona el camino actual y vuelve a la opción anterior, evitando así recorrer un camino largo e ineficiente que h(n) marcó erróneamente como el mejor.\n",
    "\n",
    "_Ejemplo:_\n",
    "\n",
    "<center><img src=\"../img/BusquedaA.jpeg\"/></center>\n",
    "\n",
    "Las heurísticas admisibles son por naturaleza optimistas, porque piensan que el coste de resolver el problema es menor que el que es en realidad. Ya que g(n) es el coste exacto para alcanzar n, tenemos como consecuencia inmediata que la f(n) nunca sobre estima el coste verdadero de una solución a través de n. \n",
    "\n",
    "_Propiedades:_\n",
    "- Completa: Sí, a menos que existan infinitos nodos con f<= f(G).\n",
    "- Temporal: Exponencial.\n",
    "- Espacio: O(b^m), mantiene todos los nodos en memoria.\n",
    "- Optima: Si."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funciones Heurísticas**\n",
    "\n",
    "Una heurística h(n) es consistente si, para cada nodo n y cada sucesor n’ de n generado por cualquier acción a, el coste estimado de alcanzar el objetivo desde n no es mayor que el coste de alcanzar n’ más el coste estimado de alcanzar el objetivo desde n’.\n",
    "\n",
    "> h(n) <= c(n,a,n’) + h(n’)\n",
    "\n",
    "Es decir,h(n) es menor o igual que el coste de cualquier camino desde n hasta el objetivo. Una heurística admisible nunca sobrestima el coste real de alcanzar una solución.\n",
    "\n",
    "_Heurística admisible_\n",
    "\n",
    "<center><img src=\"../img/HeuristicaAdmisible.jpeg\"/></center>\n",
    "\n",
    "- Si h2(n) ≥ h1(n) para todos los n (ambas admisibles) entonces h2 domina a h1.  En este caso, h2 es mejor para la búsqueda.\n",
    "- Costes de búsqueda típicos (número medio de nodos expandidos).\n",
    "- Dadas dos heurísticas admisibles ha,hb: h(n) = max(ha(n), hb(n))\n",
    "\n",
    "_Problemas relajados_\n",
    "- Un problema con menos restricciones en las acciones se denomina **problema relajado**.\n",
    "- El coste de una solución óptima en un problema relajado es una heurística admisible para el problema original.\n",
    "- **Clave**: El costo de la solución ́optima de un problema relajadono es mayor que el costo de la solución óptima del problema real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Algoritmos de búsqueda local y problemas de optimización <a class=\"anchor\" id=\"t2_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de búsquedaqua vistos hasta ahora se diseñan para explorar sistemáticamente espacios de búsqueda. Esta forma sistemática se alcana manteniendo uno o más caminos en memoria y registrando que alternativas se han explorado en cada punto o a lo largo del camino y cuáles no.\n",
    "\n",
    "Si no importa el camino al objetivo, podemos considerar una clase diferente de algoritmos, que no se preocupen en absoluto de los caminos. Los algoritmos de **búsqueda local** funcionan con un solo **estado actual** que se va optimizando. En este tipo de problemas, lo que interesa es encontrar el mejor estado (solución) de acuerdo con una función objetivo. En este sentido, el espacio de estados está formado por un conjunto completo de configuraciones que cumplen la función objetivo. \n",
    "\n",
    "Los algoritmos de búsqueda local no son sistemáticos, pero presentan dos ventajas claves:\n",
    "- Usan muy poca memoria.\n",
    "- Pueden encontrar a menudo soluciones razonables en espacios de estados grandes o infinitos para los cuales los algoritmos sistemáticos son inadecuados.\n",
    "\n",
    "Por estas ventajas, los algoritmos de **búsqueda local** son útiles para resolver problemas de **optimización**, en los cuales el objetivo es encontrar el mejor estado según una **función objetivo**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Ascensión de colinas**\n",
    "\n",
    "El algoritmo de ascensión de colinas parte de una solución y busca soluciones vecinas, de forma que coge la mejor solución vecina y la compara con la solución actual. Si la solución vecina es mejor pasa a ser la solución actual, repitiendo el proceso. Si la mejor solución vecina no mejora a la solución actual el proceso para, devolviendo la solución actual. \n",
    "\n",
    "El algoritmo de ascensión de colina es simplemente un bucle que continuamente se mueve en dirección del valor creciente, es decir, cuesta arriba. Termina cuando alcanza un pico en donde ningún vecino tiene un valor más alto. El algoritmo no mantiene un árbol de búsqueda, sino una estructura de datos del nodo actual que sólo necesita el registro del estado y su valor de función objetivo. \n",
    "\n",
    "Lamentablemente, la ascensiónde colinas a menudo se atasca por los siguientes motivos:\n",
    "- Máximo local: Un máximo local es un pico que es más alto que cada uno de sus estados vecinos pero más bajo que el máximo global.\n",
    "- Crestas: Las crestas causan una secuencia de máximos locales que hace muy difícil la navegación para los algoritmos avaros.\n",
    "- Meseta: Una meseta es un área del paisaje del espacio de estados donde la función de evaluación es plana.\n",
    "\n",
    "<center><img src=\"../img/BusquedaLocal.jpeg\"/></center>\n",
    "\n",
    "Para entender esto, consideramos la forma o el paisaje del espacio de estados, que tiene «posición» (definido por el estado) y «elevación» (definido por el valor de la función de coste heurística o función objetivo). Si la elevación corresponde al costo, entonces el objetivo es encontrar el valle más bajo (un mínimo global); si la elevación corresponde a una función objetivo, entonces el objetivo es encontrar el pico más alto (un máximo global)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Búsqueda por haz local**\n",
    "\n",
    "Guardar solamente un nodo en la memoria podría parecer una reacción extrema para el problema de limitaciones de memoria. El algoritmo de búsqueda por haz local guarda la pista de k estados (no sólo uno). Comienza por k estados (soluciones) generados aleatoriamente. \n",
    "\n",
    "En cada iteración se generan todos los estados sucesores de los k estado. Si alguno es un objetivo, entonces para. Si no, elige los mejores k sucesores de la lista completa de sucesores y repite el ciclo. \n",
    "\n",
    "El problema es que los k mejores sucesores tienen a estar en la misma zona. Como solución, se pueden elegir aleatoriamente los k mejores sucesores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Algoritmos Genéticos**\n",
    "\n",
    "Un algoritmo genético es una variante de la búsqueda de haz en la que los estados sucesores se generan combinando dos estados padres.\n",
    "\n",
    "Como en la búsqueda de haz, los algoritmos genéticos comienzan con un conjunto de k estados generados aleatoriamente, llamados población. Cada estado, o individuo, está representado como una cadena sobre un alfabeto finito (normalmente cadenas de 1s o 0s).\n",
    "\n",
    "Cada estado está tasado con una función idoneidad, la cual debería devolver valores más altos para estados mejores.\n",
    "\n",
    "En (c) se seleccionan dos pares, de manera aleatoria, para la reproducción, de acuerdo con las probabilidades en (b).  Notemos que un individuo se selecciona dos veces y uno ninguna. Para que cada par se aparee, se elige aleatoriamente un punto de cruce de las posiciones en la cadena. \n",
    "\n",
    "En (d) los descendientes se crean cruzando las cadenas paternales en el punto de cruce. Por ejemplo, el primer hijo del primer par consigue los tres primeros dígitos del primer padre y los dígitos restantes del segundo padre. \n",
    "\n",
    "<center><img src=\"../img/BAlgoGeneticos.jpeg\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Problemas de satisfacción de restricciones<a class=\"anchor\" id=\"t3\"></a>\n",
    "![title](../img/Divider.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora se explora la idea de que los problemas pueden resolverse buscando en un espacio de estados. Estos estados pueden evaluarse con heurísticas específicas del dominio y probados para ver si son estados objetivo. Desde el punto de vista del\n",
    "algoritmo de búsqueda, sin embargo, cada estado es una **caja negra** que se representa por una estructura de datos arbitraria a la que se puede acceder sólo con las rutinas.\n",
    "\n",
    "Los **problemas de satisfacción de restricciones** (PSR) ofrecen varias ventajas importantes. Dado que la representación del estado se ajusta a un modelo estándar, es decir, un conjunto de variables con valores asignados, tanto la función sucesora como la prueba de objetivo pueden escribirse de manera genérica, aplicable a cualquier PSR. Además, podemos desarrollar heurísticas eficaces y genéricas que no requieren información adicional o conocimiento experto del dominio específico. Finalmente, la estructura del grafo de restricciones puede utilizarse para simplificar el proceso de solución, lo que en algunos casos puede resultar en una reducción exponencial de la complejidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Problemas de satisfacción de restricciones <a class=\"anchor\" id=\"t3_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formalmente, un **problema de satisfacción de restricciones** (o **PSR**) está definido por\n",
    "- Un conjunto de **variables**, V1, V2…, Vn. Cada variable Vi tiene un dominio no vacío Dvi de valores posibles.\n",
    "- Un conjunto de **restricciones fuertes**, R1, R2…, Rm. Cada restricción Ri implica un subconjunto de variables y especifica las combinaciones aceptables de valores para ese subconjunto.\n",
    "\n",
    "Un estado del problema está definido por una asignación de valores a unas o todas (*formulación completa de estados*) las variables, {Vi = xi, Vj = xj…}. \n",
    "\n",
    "Buscamos una asignación de las variables que verifique la asignación objetivo.\n",
    "- **Verificación**: Debe cumplir unas restricciones entre los valores de las variables.\n",
    "- **Satisfacción de restricciones**: Las verificaciones nos dan la parte de especificidad del problema, cuanto más restricciones más especificidad.\n",
    "- **Asignación consistente o legal**: Es una asignación que no viola ninguna restricción.\n",
    "- **Asignación completa**: Es una asignación en la que se menciona cada variable.\n",
    "- **Solución o modelo**: Es una asignación de valores a todas sus variables que satisface todas las restricciones.\n",
    "Algunos\tproblemas de satisfacción de restricciones PSR\ttambién\trequiere una solución que maximiza una **función objetivo**.\n",
    "\n",
    "En estos problemas la **profundidad** viene dada por la cantidad de acciones (asignaciones) y el dominio nos da el **nivel de ramificación**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos, por ejemplo, un mapa de Australia que muestra cada uno de sus estados y territorios. Supongamos que se nos asigna la tarea de colorear cada región en rojo, verde o azul, de tal manera que ninguna región adyacente tenga el mismo color. Para formular esto como un Problema de Satisfacción de Restricciones (PSR), definimos variables para las regiones: AQ, TN, Q, NGS, V, AS y T. El dominio de cada variable es el conjunto {rojo, verde, azul}. Las restricciones exigen que las regiones vecinas tengan colores distintos.\n",
    "\n",
    "![title](../img/PRSAustralia.JPG)\n",
    "\n",
    "Es bueno visualizar\tun PSR como\tun **grafo de restricciones**, donde los nodos del grafo corresponden a\tlas\tvariables del problema y los arcos corresponden\ta las restricciones.\n",
    "\n",
    "- Varbles: WA, NT, Q, NSW, V, SA, T.\n",
    "- Dominios: {red, green, blue}.\n",
    "- Restricciones: Regiones adyacentes deben tener colores diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clases de problemas y tipos de restricciones**\n",
    "\n",
    "*Tipos de problemas*\n",
    "- La clase más simple de PSR implica variables *discretas* y *dominios finitos*\n",
    "- Las variables discretas también pueden tener dominios infinitos, implican el uso de *lenguaje de restricciones*.\n",
    "\n",
    "*Tipos de restricciones*\n",
    "- Unarias: Restringe los valores de una sola variable, por ejemplo SA *not* green.\n",
    "- Binarias: Afecta a pares de variables, por ejemplo SA not WA.\n",
    "- N-Arias: Afectan a o más variables, por ejemplo AllDiff.\n",
    "\n",
    "**Alcance:** El alcance se refiere al conjunto de variables involucradas en una restricción.\n",
    "\n",
    "**Preferencias:** (restricciones blandas): Las restricciones blandas son aquellas cuya violación no impide encontrar una solución, pero su cumplimiento es deseable para mejorar la calidad de la solución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Búsqueda con vuelta atrás para PSR <a class=\"anchor\" id=\"t3_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La \"búsqueda con vuelta atrás\" es un enfoque de búsqueda primero en profundidad que selecciona secuencialmente valores para una variable, retrocediendo cuando una variable no tiene valores legales asignables. Aunque es un algoritmo sin información y puede no ser eficiente en problemas grandes, su metodología es fundamental en la resolución de ciertos tipos de problemas (Generación de horarios, Configuración de Hardware, Planificación en transporte, Floorplanning...).\n",
    "\n",
    "![title](../img/PSRBacktracking.JPG)\n",
    "\n",
    "**Mejorando la eficiencia del backtracking**\n",
    "- ¿Qué variable debería asignar la siguiente?\n",
    "    - Pueden ser usadas una o más **heurísticas**, por ejemplo la variable mas restrictiva (Si falla que sea lo más pronto posible).\n",
    "- ¿En qué orden deberéan sus valores ser probados?\n",
    "    - Decidir sobre que valores considerar primero.\n",
    "- ¿Se podrían detectar fallos inevitables de manera temprana?\n",
    "    - Se puede realizar durante el proceso de búsqueda proporcionando una guía.\n",
    "- ¿Se podría sacar ventaja de la estructura del problema?\n",
    "    - Analizar la estructura del problema puede revelar patrones o dependencias.\n",
    "\n",
    "La heurística de **Mínimos Valores Restantes (MVR)**, también conocida como la heurística \"variable más restringida\" o \"primero en fallar\", escoge una variable que con mayor probabilidad (variable con menos valores «legales») causará pronto un fracaso, con lo cual podamos el árbol de búsqueda.\n",
    "\n",
    "La heurística de **Mínimo Grado Restante** \"grado heurístico\" (Degree heuristic), está heurístico es más práctica en la primera elección porque al principio todas las variables tienen valores legales. Intenta reducir el factor de ramificación sobre futuras opciones seleccionando la variable, entre las variables no asignadas, que esté implicada en el mayor número de restricciones, es una guía más menos poderosa, pero puede ser útil en caso de desempate MVR.\n",
    "\n",
    "Una vez que se selecciona una variable, el algoritmo debe decidir el **orden** para examinar sus valores. Para esto, la heurística del **Valor Menos Restringido** puede ser eficaz en algunos casos. Se prefiere el valor que excluye las pocas opciones de las variables vecinas en el grafo de restricciones. La heurística trata de dejar la flexibilidad máxima de las asignaciones de las variables siguientes, esto es útil si queremos encontrar todas las posibles soluciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Propagación de la información a través de las restricciones**\n",
    "\n",
    "Hasta ahora, nuestro algoritmo de búsqueda solo toma en cuenta las restricciones asociadas a una variable en el momento en que dicha variable es seleccionada. Sin embargo, al considerar algunas restricciones antes en el proceso de búsqueda, o incluso antes de que la búsqueda comience, es posible reducir de manera significativa el espacio de búsqueda. Es fundamental que este proceso sea rápido, para no aumentar el tiempo total de búsqueda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comprobación hacia delante**\n",
    "\n",
    "Otra manera para usar mejor las restricciones durante la búsqueda se llama comprobación hacia delante. Siempre que se asigne una variable X, el proceso de comprobación hacia delante mira cada variable no asignada Y que esté relacionada con X por una relación, y suprime del dominio de Y cualquier valor que sea inconsistente con el valor elegido para X.\n",
    "\n",
    "![title](../img/PRSCHD.JPG)\n",
    "\n",
    "Hay dos puntos importantes que debemos destacar sobre este ejemplo. Primero, notemos que después de asignar AQ = rojo y Q = verde, los dominios de TN y AS se reducen a un solo valor; hemos eliminado las ramificaciones de estas variables totalmente propagando la información de AQ y Q. La heurística MVR, la cual es una compañera obvia para la comprobación hacia delante, seleccionaría automáticamente AS y TN después (en efecto, podemos ver como la comprobación hacia delante como un modo eficiente de incrementar el cálculo de la información que necesita la heurística MVR para hacer su trabajo). Un segundo punto a tener en cuenta es que, después de V = azul, el dominio de SA está vacío. Por eso, la comprobación hacia delante ha descubierto que la asignación parcial {AQ = rojo, Q = verde, V = azul} es inconsistente con las restricciones del problema, y el algoritmo volverá atrás inmediatamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arcos Consistentes**\n",
    "\n",
    "La técnica de \"arco consistente\" es un método eficaz de propagación de restricciones que es más poderoso que la comprobación hacia delante. Consiste en asegurarse de que para cada valor posible de una variable, haya una asignación consistente en las variables relacionadas.\n",
    "\n",
    "- Idea: Podar los dominios tanto como sea posible antes de seleccionar valores.\n",
    "- Variable es consistente con su dominio: Ningún valor del dominio del nodo es clasificado como imposible por ninguna de las restricciones.\n",
    "- Fácil de identificar en restricciones unarias pero muy complicado en k-arias.\n",
    "\n",
    "\n",
    "**Red de Restricciones**\n",
    "- Hay un nodo ovalado para cada variable.\n",
    "- Hay un nodo rectangular para cada restricción.\n",
    "- Hay un dominio de valores asociado a cada nodo.\n",
    "- Hay un arco desde cada variable X a cada restricción que involucra a X.\n",
    "\n",
    "![title](../img/PSRRdR.JPG)\n",
    "\n",
    "- Un arco es consistente: Se dice que es consistente si para cada valor posible de X (x) hay al menos un valor en Y (y) que cumpla con la regla r(x, y).\n",
    "- Una red es arco consistente si todos sus arcos son consistentes.\n",
    "- Si se encuentra un arco que no es consistente (es decir, hay valores en X para los cuales no hay valores correspondientes compatibles en Y), se puede hacer consistente eliminando esos valores problemáticos del conjunto de posibles valores de X.\n",
    "\n",
    "![title](../img/PSRArco.JPG)\n",
    "\n",
    "La comprobación de la consistencia de arcos puede implementarse tanto como un paso de preproceso antes de iniciar la búsqueda, como también durante la búsqueda, similar a la comprobación hacia delante, después de cada asignación. El algoritmo completo para mantener la consistencia de arcos, conocido como AC-3, utiliza una cola para administrar los arcos pendientes de revisión. Cada arco (Xi, Xj) se examina para determinar si es necesario eliminar valores del dominio de Xi; si es así, todos los arcos que apuntan a Xi se agregan de nuevo a la cola para ser reevaluados.\n",
    "\n",
    "Un grafo es fuertemente k-consistente si cumple con la condición de k-consistencia y todas las condiciones de consistencia inferiores, es decir, (k-1)-consistencia, (k-2)-consistencia, y así sucesivamente, hasta 1-consistencia.\n",
    "\n",
    "![title](../img/PSRAC3.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Resolución por Búsqueda Local <a class=\"anchor\" id=\"t3_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de búsqueda local son muy efectivos para resolver muchos Problemas de Satisfacción de Restricciones (PSR). Utilizan una formulación de estado completa, donde el estado inicial asigna un valor a cada variable y la función sucesora cambia el valor de una variable a la vez. Por ejemplo, en el problema de las 8-reinas, se podría comenzar con una configuración aleatoria de las reinas y mover una reina a otra posición en su columna como sucesor.\n",
    "\n",
    "![title](../img/PSRMinConflictos.JPG)\n",
    "\n",
    "Una estrategia común en la búsqueda local es seleccionar el nuevo valor de una variable que cause el mínimo de conflictos con otras variables, conocida como la heurística de mínimos conflictos. Esta heurística ha demostrado ser sorprendentemente efectiva, especialmente si el estado inicial es razonablemente bueno.\n",
    "\n",
    "Interesantemente, en el problema de las n-reinas, el tiempo de ejecución del algoritmo de mínimos conflictos es casi independiente del tamaño del problema, resolviendo problemas de hasta un millón de reinas en un promedio de 50 pasos después de la asignación inicial.\n",
    "\n",
    "![title](../img/PSRMinConReinas.JPG)\n",
    "\n",
    "Este hallazgo fue un estímulo clave en la investigación de los años 90 sobre la búsqueda local y la distinción entre problemas fáciles y difíciles. En términos generales, el problema de las n-reinas es más accesible para la búsqueda local porque las soluciones están densamente distribuidas en el espacio de estados. La heurística de mínimos conflictos también es efectiva en problemas más desafiantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Búsqueda entre adversarios <a class=\"anchor\" id=\"t4\"></a>\n",
    "![title](../img/Divider.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estos problemas los agentes deben considerar las acciones de otros y su impacto en su propio bienestar. La imprevisibilidad de otros agentes añade complejidad al proceso de resolución de problemas. Se distingue entre entornos multiagente cooperativos y competitivos, siendo los últimos un escenario para problemas de búsqueda entre adversarios o juegos.\n",
    "\n",
    "Los juegos son un área de interés en IA debido a su naturaleza abstracta y reglas definidas. Han sido un foco desde los inicios de la IA, con avances significativos en juegos como ajedrez y damas. A pesar de su aparente simplicidad, los juegos representan desafíos complejos y sirven para desarrollar técnicas como la poda y evaluaciones heurísticas, aplicables en situaciones donde no es factible calcular la decisión óptima. El capítulo explora también juegos con elementos de azar e información imperfecta, y concluye examinando el rendimiento de los programas de IA en juegos contra humanos y posibles direcciones futuras de investigación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Decisiones\tóptimas <a class=\"anchor\" id=\"t4_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideramos juegos\tcon\tdos\tjugadores, que llamaremos MAX y\tMIN\tpor\tmotivos\tque\tpronto se harán evidentes. MAX mueve primero, y\tluego mueven por turno hasta que el juego termina. Al final del juego, se conceden puntos al jugador ganador y penalizaciones al perdedor. Un juego puede definirse\tformalmente\tcomo una clase de problemas\tde búsqueda\tcon\tlos\tcomponentes\tsiguientes:\n",
    "- El **estado inicial**, que incluye la posición del tablero e identifica al jugador que mueve.\n",
    "- Una **función sucesor**, que devuelve una lista de pares (movimiento, estado), indicando una **acción o movimiento** legal y el estado que resulta.\n",
    "- Un **test terminal**, que determina cuándo se termina el juego. A los estados donde el juego se ha terminado se les llaman estados terminales.\n",
    "- Una **función utilidad** (también llamada función objetivo o función de rentabilidad), que da un valor numérico a los estados terminales (Victoria +1, empate 0, derrota -1).\n",
    "\n",
    "![title](../img/MinMax.JPG)\n",
    "\n",
    "El estado inicial y los movimientos legales para cada lado definen el **árbol de juegos**. La imagen muestra la parte del árbol de juegos para el tres en raya. Desde el estado inicial, MAX tiene nueve movimientos posibles. El juego alterna entre la colocación de una X para MAX y la colocación de un O para MIN, hasta que alcancemos nodos hoja (Victoria, empate, derrota) correspondientes a estados terminales.\n",
    "\n",
    "En un problema de búsqueda normal, la solución óptima sería una secuencia de movimientos que conducen a un estado objetivo (un estado terminal que es ganador). En un juego, por otra parte, MIN tiene algo que decir sobre ello. MAX, por lo tanto, debe encontrar una **estrategia** contingente, que especifica el movimiento de MAX en el estado inicial, después los movimientos de MAX en los estados que resultan de cada respuesta posible de MIN, después de los movimientos de MAX en los estados que resultan de cada respuesta posible de MIN de los anteriores movimientos, etc.\n",
    "\n",
    "Incluso un juego simple como el tres en raya es demasiado complejo para dibujar el árbol de juegos entero, por tanto, cambiemos al juego trivial de la imagen 2. Los movimientos posibles para MAX, en el nodo raíz, se etiquetan por a1, a2 y a3. Las respuestas posibles a a1, para MIN, son b1, b2, b3, etc. Este juego particular finaliza después de un movimiento para MAX y MIN.\"\n",
    "\n",
    "![title](../img/MinMaxTreeSimp.JPG)\n",
    "\n",
    "- Jugada perfecta para juegos determinísticos con información perfecta.\n",
    "- Idea: Elegir movimientos a posciones con el mayor valor minimax.\n",
    "- Valor minmax: Mayor recompensa alcanzable contra mejor jugada.\n",
    "- Nodo raíz (Max): Mi primera jugada.\n",
    "- El árbol se construye por niveles yo, adversario, yo...\n",
    "- En los niveles Min pierdo el control, depende de mi adversario.\n",
    "- Los nodos finales se etiquetan por sus valores de utilidad.\n",
    "\n",
    "\n",
    "**El algoritmo minimax**\n",
    "El algoritmo minimax calcula la\tdecisión minimax del estado actual. Usa un cálculo simple recurrente de\tlos valores minimax de cada estado sucesor, directamente implementando las ecuaciones de la\tdefinición. La recursión avanza\thacia las hojas\tdel\tárbol, y entonces los valores minimax retroceden por el\tárbol cuando la\trecursión seva deshaciendo.\n",
    "\n",
    "![title](../img/MinMaxAlgo.JPG)\n",
    "\n",
    "*Propiedades*\n",
    "- **Completo:** Sí, si el árbol es finito.\n",
    "- **Óptimo:** Sí, contra un oponente óptimo.\n",
    "- **Complejidad temporal:** O(b^m).\n",
    "- **Complejidad espacial:** O(b*m) exploración primero en profundidad.\n",
    "- Para ajedrez b = 35, m = 100.\n",
    "\n",
    "![title](../img/DiagramaAlfa-Beta.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Minimax para múltiples jugadores***\n",
    "\n",
    "Muchos juegos populares permiten más de dos jugadores. Examinemos cómo obtener una extensión de la idea minimax a juegos multi-jugador. Esto es sencillo desde el punto de vista técnico, pero proporciona algunas nuevas cuestiones conceptuales interesantes.\n",
    "\n",
    "Primero, tenemos que sustituir el valor de cada nodo con un vector de valores. Por ejemplo, en un juego de tres jugadores con A, B y C, un vector {Va, Vb, Vc} asociado con cada nodo. Para los estados terminales, este vector dará la utilidad del estado desde el punto de vista de cada jugador. (En dos jugadores, en juegos de suma cero, el vector de dos elementos puede reducirse a un valor porque los valores son siempre opuestos). El camino más simple de implementarlo es hacer que la función UTILIDAD devuelva un vector de utilidades.\n",
    "\n",
    "![title](../img/MinMaxMult.JPG)\n",
    "\n",
    "- Todos los **jugadores** son **Max**.\n",
    "- La función de **evaluación** viene dada por un **vector**.\n",
    "- Cada nivel está **asignado** a un **jugador**.\n",
    "- Turnos cada n **niveles** para n jugadores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Poda Alfa-Beta <a class=\"anchor\" id=\"t4_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema de la búsqueda minimax es que el número de estados que tiene que examinar es exponencial en el número de movimientos. Es posible calcular la decisión minimax correcta sin mirar todos los nodos en el árbol de juegos, decir podemos tomar **podar** a fin de eliminar partes grandes del árbol.\n",
    "\t\n",
    "Cuando lo aplicamos\ta un árbol minimax estándar, devuelve el mismo movimiento que devolvería minimax, ya que podar las ramas no\tpuede influir, posiblemente, en\tla decisión final.\n",
    "\n",
    "![title](../img/PodaAlfa-Beta.JPG)\n",
    "\n",
    "La poda alfa-beta consigue su nombre de los dos parámetros que describen los límites sobre los valores hacia atrás que aparecen a lo largo del camino:\n",
    "- Alfa: El valor de la mejor opción (es decir, valor más alto) que hemos encontrado hasta ahora en cualquier punto elegido a lo largo del camino para MAX.\n",
    "- Beta: El valor de la mejor opción (es decir, valor más bajo) que hemos encontrado hasta ahora en cualquier punto elegido a lo largo del camino para MIN\n",
    "\n",
    "![title](../img/PodaAlfaBeta2.JPG)\n",
    "\n",
    "***Propiedades de Alfa-Beta***\n",
    "- La **poda** no afecta al resultado **final.**\n",
    "- Una **ordenación** concreta de los posibles **movimientos** mejora la efectividad de la **poda.**\n",
    "- Con esa \"ordenación perfecta\" la complejidad **temporal** O(b^m/2) duplica la profundidad solucionable.\n",
    "- Desafortunadamente 35^50 es aún imposible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Funciones de evaluación <a class=\"anchor\" id=\"t4_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo minimax genera el espacio de búsqueda entero, mientras que el algoritmo alfa-beta permite que podemos partes grandes de él. Sin embargo, alfa-beta todavía tiene que buscar en todos los caminos (**limitación de recursos**), hasta los estados terminales, para una parte del espacio de búsqueda. Esta profundidad no es, por lo general, práctica porque los movimientos deben hacerse en una **cantidad razonable de tiempo**. El trabajo de Shannon en 1950 puso un cambio: que los programas deberían cortar la búsqueda antes y entonces aplicar\n",
    "una función de evaluación heurística a los estados, convirtiendo efectivamente, los nodos no terminales en hojas terminales. Se sustituye la función de utilidad por una **función de evaluación** heurística EVAL, que da una estimación de la utilidad de la posición, y se sustituye el test-terminal por un **test-límite** que decide cuándo aplicar EVAL.\n",
    "\n",
    "\n",
    "Una\tfunción de evaluación devuelve una **estimación de la utilidad** esperada de una posición dada.\n",
    "- Primero, la función de evaluación debería ordenar los estados terminales del mismo modo que la función de utilidad verdadera.\n",
    "- Segundo, el cálculo no debe utilizar demasiado tiempo (el jugador no desea esperar días hasta que el algoritmo obtenga la mejor jugada posible).\n",
    "- Tercero, para estados no terminales, la función de evaluación debería estar fuertemente correlacionada con las posibilidades actuales de ganar.\n",
    "\n",
    "El siguiente paso es modificar la búsqueda Alfa-Beta de modo que llame a la función heurística EVAL cuando se corte la búsqueda, es decir incluir:\n",
    "\n",
    "![title](../img/FuncionEval.JPG)\n",
    "\n",
    "También debemos llevar la contabilidad de la profundidad de modo que la profundidad actual se incremente sobre cada llamada recursiva. La aproximación más sencilla es poner algún límite de profundidad fijo, la profundidad se elige de modo que la cantidad de tiempo usado no exceda de lo que permiten las reglas del juego.\n",
    "\n",
    "\n",
    "***Juegos que incluyen un elemento de posibilidad***\n",
    "\n",
    "En la vida real, hay muchos acontecimientos imprevisibles externos que nos ponen en situaciones inesperadas. Los juegos de azar pueden manejarse con una extensión del algoritmo minimax que evalúa un nodo de posibilidad tomando la utilidad media de todos sus nodos hijos.\n",
    "\n",
    "Calcular el valor esperado, donde la expectativa se toma sobre todos los posibles resultados que podrían ocurrir. Éste nos conduce a generalizar el valor minimax para juegos deterministas a un valor minimaxesperado para juegos con nodos de posibilidad.\n",
    "\n",
    "El juego óptimo en juegos de información imperfecta, como el bridge, requiere el razonamiento sobre los estados de creencia actuales y futuros de cada jugador. Una aproximación simple puede obtenerse haciendo un promedio del valor de una acción sobre cada configuración posible de la información ausente.\n",
    "\n",
    "***Programas de juegos***\n",
    "\n",
    "Podríamos decir que jugar a juegos es a IA como el Gran Premio de carreras de automóviles es a la industria de coches: los programas de juegos son deslumbrantemente rápidos, máquinas increíblemente bien ajustadas que incorporan técnicas muy avanzadasde la ingeniería, pero no son de mucho uso para hacer la compra. \n",
    "Aunque algunos investigadores crean que jugar a juegos es algo irrelevante en la corriente principal de IA, se sigue generando entusiasmo y una corriente estable de innovaciones que se han adoptado por la comunidad:\n",
    "\n",
    "-Damas: Chinook terminó con 40 años de reinado del campeón mundial Marion Tinsley en 1994. Usaba un base de datos con configuraciones finales definiendo la jugada perfecta para todas las posiciones en las que restaban 8 o menos piezas en el tablero, un total de 44.374.401.247 posiciones.\n",
    "- Chess: Deep Blue derrotó al campeón mundial Gary Kasparov en una competición a seis partidas en 1997. Deep Blue busca 200 millones de posiciones por segundo, usando una evaluación muy sofisticada y métodos no revelados para extender algunas líneas de búsqueda a nivel superior a 40.\n",
    "- Othello: Campeones rehusaron competir contra computadores, porque eran demasiado buenos.\n",
    "- Go: Campeones rehusaron competir contra computadores, por jugar muy mal. En go, b > 300, así que la mayoría de programas usan bases de conocimiento de patrones para sugerir posibles movimientos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aprendizaje <a class=\"anchor\" id=\"t5\"></a>\n",
    "![title](../img/Divider.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lo largo de nuestra vida, realizamos una serie de acciones para perseguir nuestros sueños. Algunas de ellas nos traen buenas recompensas y otras no. A lo largo del camino, seguimos explorando diferentes caminos e intentamos averiguar qué acción puede llevarnos a obtener mejores recompensas.\n",
    "\n",
    "El aprendizaje es la **habilidad** de mejorar el **comportamiento** propio a base de experiencia:\n",
    "- El rango de comportamientos puede ser expandido: el agente puede hacer más.\n",
    "- La precisión en tareas es mejorada: el agente puede hacer las cosas mejor.\n",
    "- La velocidad se mejora: el agente puede hacer cosas más rápido.\n",
    "\n",
    "Un agente de aprendizaje puede ser diseñado con un **elemento de acción**, que decide qué acciones llevar a cabo y con un **elemento de aprendizaje**, que modifica el elemento de acción para poder tomar **mejores decisiones**.\n",
    "\n",
    "Los siguientes componentes son parte de un problema de aprendizaje:\n",
    "- **Tarea:** El comportamiento o tarea que ha de ser **mejorado**.\n",
    "    - Qué componentes del elemento de acción tienen que aprenderse.\n",
    "- **Datos:** Las **experiencias** que se utilizan para mejorar el rendimiento de la tarea.\n",
    "    - Qué tipo de representación se usa para los componentes.\n",
    "- **Medida de Mejora:** ¿Cómo se puede medir la mejora?\n",
    "    - Qué realimentación está disponible para aprender dichos componentes.\n",
    "\n",
    "El **tipo de realimentación** disponible para el aprendizaje normalmente es el factor más importante a la hora de determinar la naturaleza del problema de aprendizaje que tiene que afrontar el ajente. Se distinguen tres tipos de aprendizaje:\n",
    "- **Supervisado:** Lo que tiene que ser aprendido es especificado para cada ejemplo.\n",
    "- **No supervisado:** No se da ninguna clasificación; El agente debe descubrir categorías y regularidades en los datos.\n",
    "- **Por refuerzo:** La retroalimentación sucede despuésde una secuencia de acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Aprendizaje Supervisado <a class=\"anchor\" id=\"t5_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducir y generalizar los datos que conozco. Función de clasificación c(x, clase):\n",
    "- x: Información que describe el dato.\n",
    "- clase: Etiqueta que describe a que grupo pertenece.\n",
    "- etiqueta: Cuestión de eficiencia.\n",
    "\n",
    "Sin un buena etiquetación sería imposible tener un clasificador, si conozco a que clase pertenece (similitud) cada dato puedo aplicar una serie de características.\n",
    "\n",
    "**Función de clasificación c(x, clase) -> Clase a la que pertenece -> Aprendizaje supervisado**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Aprendizaje no Supervisado <a class=\"anchor\" id=\"t5_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se establece una función de distancia d(x, y) relación entre dos individuos, para calcular esta similitud podemos utilizar diferentes funciones (distancia vectorial, coseno, euclídea...).\n",
    "\n",
    "**Función de distancia d(x, y) -> Agrupamiento (Clustering) -> Aprendizaje no supervisado**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Aprendizaje por Refuerzo <a class=\"anchor\" id=\"t5_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprendizaje basado en prueba y error (experiencia), de tipo no supervisado, aunque se puede hacer una evaluación externa.\n",
    "\n",
    "Se obtiene una **retroalimentación/refuerzo** (efecto de mis acciones) del entorno, buscando así el máximo de refuerzo en el tiempo, por eso será muy importante poder simular las acciones.\n",
    "\n",
    "Podemos con una **función de transición de de probabilidad** T(s,a,s') = P(s'|s,a) podemos alcanzar más de un estado.\n",
    "\n",
    "Además tendremos una **función de recompensa** r(s) ganancia de alcanzar el estado, obtenemos así una secuencia de estados (Proceso de Markov), donde un estado sólo depende del anterior.\n",
    "\n",
    "Para resolver esto introducimos una **política** que guiará las acciones que debo tomar en cada momento.\n",
    "\n",
    "Un algoritmo con política (on-polish) realizará una evaluación de los estados en su rendimiento para optener la política óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Componentes de un problema de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es el más general de las tres categorías. Los agentes son programados por medio de **recompensas** y **castigos** se especifica **qué** hacer, pero **no cómo** hacerlo. \n",
    "\n",
    "Dado el entorno: \n",
    "\n",
    "![title](../img/EntornoRL.JPG)\n",
    "\n",
    "**Definición del planteamiento del problema**\n",
    "- **Estado:** El lugar en el que se encuentra un agente concreto en un momento determinado indica su estado.\n",
    "- **Acciones:** Ubicaciones (estados) directas a las que puede ir un robot desde una ubicación determinada.\n",
    "- **Transiciones:** Se pueden describir mediante una función de transición T(s, a, s′), donde a es una acción ejecutable en el estado actual s, siendo s′ algún nuevo estado.\n",
    "- **Recompensa:** La retroalimentación es simplemente un valor **escalar** que puede **retrasarse** en el tiempo.\n",
    "\n",
    "![title](../img/TablaEstados.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema de decisión secuencial. Procesos de decisión de Markov**\n",
    "\n",
    "En los problemas de decisiones secuenciales, la utilidad de las\n",
    "acciones del agente no **dependen** de decisiones **individuales**, sino  toda la **secuencia** de las acciones del agente.\n",
    "\n",
    "Supongamos que el agente necesita alcanar L6 desde L1. Para conseguir esto, una idea sería introducir algún tipo de huella que el agente pudiera seguir.\n",
    "\n",
    "Si el problema es **determinista** (dirección prefijada) y el conocimiento del agente de su posición es completo (observable), entonces el problema se reduciría a un problema de **planificación** de acciones.\n",
    "\n",
    "Cuando tenemos huellas en dos direcciones diferentes, es incapaz de decidir qué camino tomar (**problemas de decisiones secuenciales**). Esto ocurre principalmente porque el agente no tiene forma de recordar las direcciones a seguir. Por tanto, nuestro trabajo ahora consiste en dotar al robot de memoria.\n",
    "\n",
    "Existe un paradigma de programación desarrollado especialmente para resolver problemas que contienen subproblemas repetitivos. Se trata de la **programación dinámica**. Fue inventado por Richard Bellman en 1954. Aquí es donde entra en juego la ecuación de Bellman:\n",
    "\n",
    "![title](../img/BellmanEquation.JPG)\n",
    "\n",
    "- **s:** Un estado en particular.\n",
    "- **a:** Una acción.\n",
    "- **s':** Estado prima que alcanzamos a partir de s.\n",
    "- **factor de descuento (𝜸)** notifica al robot lo lejos que está del destino. Determina la importancia de las recompensas futuras, factor 0 < f < 1 conforme pasa el tiempo considero menos las acciones, se hace más cercano a 0.\n",
    "- **R(s,a):** Función recompensa.\n",
    "- **V(s):** Valor de recompensa en un estado en particular.\n",
    "- **max()** Está función ayuda a elegir siempre el estado que da el máximo valor de estar en ese estado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entorno Estocástico**\n",
    "\n",
    "Si somos realistas siempre hay algo de estocasticidad, nuestro entorno no siempre funciona como esperamos los resultados de una acción son inciertos y las recompensas por actuar no pueden obtenerse hasta que no hayan transcurrido un número suficiente de acciones. \n",
    "\n",
    "Esto quiere decir que el agente puede acabar en un estado distinto al objetivo. Por lo tanto, cualquier solución debe especificarse de modo que el agente siempre sepa qué hacer en cada estado posible que pueda alcanzar.\n",
    "\n",
    "¿Cómo introducimos esta estocasticidad en nuestro caso? Procesos de decisión de Markov (MDP) extenión del modelo de Markov estándar.\n",
    "\n",
    "Estos **procesos de decisión de Markov** (PDM) proporciona un marco matemático para modelar la toma de decisiones en situaciones en las que los resultados son en **parte aleatorios** y en **parte bajo control**.\n",
    "Se definen mediante un **modelo de transición** T(s, a, s′), que especifica los resultados probabilistas de cada acción y una función de recompensa, que determina la recompensa en cada estado.\n",
    "\n",
    "La **utilidad** de una secuencia de estados viene dada por la **suma de todas las posibles recompensas R** sobre la secuencia.\n",
    "\n",
    "Con objetivo de **maximizar** el potencial valor de R' en el futuro. Se desea **encontrar** una política (**mapeo** de **estados** a **acciones**).\n",
    "\n",
    "Se denomina **política** a una solución de este tipo, es la función que nos permite calcular la siguiente acción en un estado concreto. La idea es mantener la política dando vueltas tanto tiempo como el rendimiento mejore, y después parar.\n",
    "- **Política apropiada:** Política que garantiza alcanzar un estado terminal.\n",
    "- **Política completa:** No importa el resultado de una acción, el agente siempre sabe qué hacer a continuación.\n",
    "- **Política óptima Π\\*:** Es la política que maximiza la recompensa esperada.\n",
    "\n",
    "Por tanto, la tarea de RL es usar las recompensas observadas para encontrar una **política óptima** para el agente en el entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Principales conceptos:***\n",
    "- **Aprendizaje por Refuerzo Pasivo**: La política de los agentes es **fija** en un entorno completamente observable y nuestra tarea es aprender la bondad de la política.\n",
    "- **Aprendizaje por Refuerzo Activo**: Los agentes **modifican sus políticas** conforme van aprendiendo.\n",
    "- **Aprendizaje basado en modelos**: Arender las **transiciones** y las **recompensas**, usarlas para conseguir la política óptima.\n",
    "- **Aprendizaje libre de modelos**: Obtiene la política óptima sin **aprender el modelo**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Exploración vs. Explotación***\n",
    "- **Explotación:** Utilizar el conocimiento ya aprendido para decidir\n",
    "cuál es la mejor acción en el estado actual.\n",
    "- **Exploración:** Para mejorar las políticas el agente debe explorar\n",
    "nuevos estados i.e. seleccionar una acción diferente a la que se piensa que es la mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dificultades en RL***\n",
    "- **Problema de Atribución de la Culpa:** El problema de determinar que acción era responsable de una recompensa o castigo.\n",
    "- **Reconocer las recompensas retrasadas:** Lo que pueden parecer acciones pobres pueden dirigir a recompensas mucho más importantes en el futuro que aquellas que parecen ser mejores acciones.\n",
    "- **Dilema Explorar-Explotar:** Si el agente ha conseguido identificar\n",
    "una buena secuencia de acciones, debería seguir estas acciones o debería explorar para intentar encontrar acciones mejores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Iteración de Valores***\n",
    "\n",
    "Buscamos una política óptima, la idea básica es la de calcular la utilidad de cada estado. La utilidad de los estados se define en función de la utilidad de las secuencias de estados. Obviamente, las secuencias de estados dependen de la política que se ejecute.\n",
    "\n",
    "Las **consecuencias** de las acciones (recompensas futuras) y los efectos de las acciones no son conocidas de manera **inmediata**. Por tanto, se necesitan mecanismos para controlar y ajustar la política cuando la recompensa para el espacio de estados actual es incierta.\n",
    "\n",
    "Está ecuación es conocida como **Ecuación de Bellman** es la base del algoritmo de iteración de valores. Comienza aplicando una **política inicial** y de manera iterativa refina V(s) hasta que se alcanza un nivel de convergencia aceptable.\n",
    "\n",
    "![title](../img/FunValorMax.JPG)\n",
    "\n",
    "El algoritmo de iteración de políticas alterna los dos pasos siguientes, partiendo de alguna política inicial:\n",
    "- Evaluación de la política: Calcula la utilidad de cada estado fuese ejecutado.\n",
    "- Mejora de la política: Calcula una nueva política empleando anticipación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q-Lerning o Aprendizaje de una Función Acción-Valor***\n",
    "\n",
    "Además de la función estado-valor, es conveniente para los algoritmos de RL añadir otra función estado-acción Q(s, a).\n",
    "\n",
    "El Q-Learning plantea una idea de **evaluación** de la **calidad** de una acción que se realiza para pasar a un estado en lugar de determinar el posible valor del estado al que se pasa.\n",
    "\n",
    "Nuestro objetivo es maximizar la función de valor Q para encontrar la política óptima. Q-function óptima Q*(s, a) representa la recompensa total esperada por un agente comenzando en s y escogiendo la acción a.\n",
    "\n",
    "Las funciones-Q son otra forma de almacenar información de utilidad, pero tienen una propiedad importante: no necesitan un modelo ni para el aprendizaje ni para la selección de acciones. Por esta razón, el aprendizaje-Q se denomina **método libre de modelo** (model-free). El agente descubrirá cuáles son las buenas y malas acciones mediante ensayo y error.\n",
    "\n",
    "Sabemos que el entorno es estocástico por naturaleza y que la recompensa que obtendrá el robot tras pasar al estado superior puede ser diferente de una observación anterior. Entonces, ¿cómo capturamos este cambio (léase diferencia)? Volvemos a calcular el nuevo Q(s, a) con la misma fórmula y le restamos el Q(s, a) conocido anteriormente.\n",
    "\n",
    "Esta aproximación se conoce como Aprendizaje de Diferencia Temporal.\n",
    "\n",
    "![title](../img/QLearningTD.JPG)\n",
    "\n",
    "La ecuación proporciona la diferencia temporal en los valores Q, lo que ayuda aún más a captar los cambios aleatorios que puede imponer el entorno. \n",
    "\n",
    "![title](../img/QLearningNewQSA.JPG)\n",
    "\n",
    "- **ɑ**: Es la tasa de aprendizaje que controla la rapidez con la que el agente se adapta a los cambios del entorno.\n",
    "- **Qt(s,a)**: Valor Q actual.\n",
    "- **Qt-1(s,a)**: Es el valor Q registrado anteriormente.\n",
    "\n",
    "Si sustituimos TDt(s,a) obtenemos la equación completa:\n",
    "\n",
    "![title](../img/QLearningFullEq.JPG)\n",
    "\n",
    "\n",
    "En resumen \n",
    "Q-Table es sólo un nombre elegante para una simple tabla de búsqueda donde calculamos las máximas recompensas futuras esperadas por acción en cada estado.\n",
    "\n",
    "Hay un proceso iterativo de actualización de los valores. A medida que comenzamos a explorar el entorno, la Q-function nos da mejores y mejores aproximaciones, actualizando continuamente los Q-values de la tabla.\n",
    "\n",
    "***Algoritmo Q-Learning***\n",
    "1. **Inicializar la Q-Table**: Hay n columnas (número de acciones) y m filas (número de estados) todos inicializados a 0.\n",
    "2. **Elegir y realizar una acción**: Elegimos una acción, peor como se menciono cada valor Q es 0. Así que ahora entra en juego el concepto de compensación de exploración y explotación. A medida que el agente explora el entorno, la velocidad de épsilon disminuye y el robot comienza a explotar el entorno.\n",
    "3. **Evaluar recompensa y actualizar Q**: Ahora hemos tomado una acción y observado un resultado y recompensa.Necesitamos actualizar la función Q(s,a)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
